<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">
  <link rel="shortcut icon" href="img/favicon.ico">

  <title>AM 207 Final Project</title>

  <!-- Bootstrap core CSS -->
  <link href="//netdna.bootstrapcdn.com/bootswatch/3.1.1/yeti/bootstrap.min.css" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="http://getbootstrap.com/examples/jumbotron-narrow/jumbotron-narrow.css" rel="stylesheet">

  <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
      <![endif]-->

      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$','$']]}
        });
      </script>
      <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
      </script>
    </head>

    <body>

      <div class="container">
        <div class="header">
          <ul class="nav nav-pills pull-right">
            <li><a href="index.html">Home</a></li>
            <li class="active"><a href="results.html">Results</a></li>
            <li><a href="https://github.com/pjbull/civil_conflict/" target="_blank">
              <img src="img/GitHub-Mark-32px.png" height="18" style="vertical-align:text-top;"/> Code</a></li>
            </ul>
            <h3 class="text-muted">AM 207 Final Project</h3>
          </div>

          <div class="container">
            <h2>Modeling and Simulating Political Violence and Optimizing Aid Distribution in Uganda</h2>
            <h3 class="text-muted">Results</h3>
          </div>

          <div class="row marketing">
            <div class="col-md-12">

              Using MCMC techniques, we model civil conflict in Uganda. We describe a method to simulate civil conflict events in space and time given historical data about these events. We also optimize the delivery of humanitarian aid as a combination of the traveling salesman problem and the Knapsack problem &mdash; two NP-hard problems &mdash; we find acceptable solutions using stochastic metaheuristics.

              <h3>Background</h3>

              The data comes from ACLED (Armed Conflict Location and Event Data Project), which is a dataset with locations, dates, fatalities,
              motivation, actors involved, and other information about civil conflicts in Africa. Their collection of data on Uganda covers 1997-2013, and they have a real-time tracker of events reported in 2014. The need for an understanding of these patterns of conflict is clear, as ACLED notes:

              <p></p>
              <blockquote>
                This dataset codes the dates and locations of all reported political violence events in over 50 developing countries. Political violence includes events that occur within civil wars or periods of instability. Although civil war occurrence is decreasing across African countries, new forms of political violence are becoming more common.
              </blockquote>
              <p></p>

              In this project, we will focus on the Republic of Uganda, for which the dataset contains around 4,500 observations of civil violence. Each observation includes the date, geographic location, event type, number of fatalities for the event, actors involved and a meta-measure which estimates how precise these measures are. Figure <a href="#1">1</a> shows these conflicts scattered on a political map of Uganda.

              <p></p>
              <div class="panel panel-info">
                <a name="1"></a>
                <div class="panel-heading">Figure 1</div>
                <div class="panel-body">
                  <img src="img/uganda.png" width="100%" max-width="600" />
                </div>
                <div class="panel-footer">Civil conflicts in Uganda 1997-2013.</div>
              </div>

              <h3>Modeling civil conflict</h3>

              <h4>Events in space</h4>

              The violence and civil conflict events described in the ACLED dataset are coded with both a latitude and a longitude. One assumption we make is that these events are distributed due to underlying causes such as population centers, road access, historical land ownership, and other features that make conflicts more or less likely. It is difficult to directly create a generative model for these probabilities, so we will make the further assumption that the distribution of the data already incorporates and is representative of these factors. In effect, we treat the entire country of Uganda as a probability distribution from which geospatial conflict events could be sampled. We took historical conflict location data from the entire ACLED data set and smoothed it using a Matérn covariance function. Figure <a href="#2">2</a> shows this smoothing applied to the same conflicts depicted in figure <a href="#1">1</a>.

              <p></p>
              <div class="panel panel-info">
                <a name="2"></a>
                <div class="panel-heading">Figure 2</div>
                <div class="panel-body">
                  <img src="img/map-with-smoothed-data.png" width="100%" max-width="600" />
                </div>
                <div class="panel-footer">Conflicts with Matérn smoothing.</div>
              </div>


              We used this smooth function as a kernel-density esitmate (KDE). This estimate (i.e., the empirical distribution of the conflict data), has a complex functional form which makes it challenging to sample from.
              However, for any given coordinate it is quite simple to get the probability of an event. Given this property of our KDE, we can apply Monte Carlo sampling techniques to generate samples from this probability distribution. Visualizing the distribution, we can see that it is multi-modal with regions of low density between the modes. Because of these properties, we opted to use slice sampling to generate draws from the distribution. Figure <a href="#3">3</a> shows the first 1,000 samples from this probability distribution. (Note: we throw away samples that occur over water or outside of country boundaries.)

              Figure <a href="#4">4</a> shows the distribution of the samples as a two-dimensional histogram.

              <p></p>
              <div class="panel panel-info">
                <a name="3"></a>
                <div class="panel-heading">Figure 3</div>
                <div class="panel-body">
                  <img src="img/1000-slice-samples.png" width="100%" max-width="600" />
                </div>
                <div class="panel-footer">Blue dots are samples from the empirical distribution.</div>
              </div>

               

              <p></p>
              <div class="panel panel-info">
                <a name="4"></a>
                <div class="panel-heading">Figure 4</div>
                <div class="panel-body">
                  <img src="img/histogram.png" width="100%" max-width="600" />
                </div>
                <div class="panel-footer">Histogram of slice samples from the empirical distribution.</div>
              </div>

              When slice sampling, there are a number of parameter choices that are imporant. We want to choose our rectangle widths, burn-in, and thinning parameters appropriately. In testing, a thinning value of 10 reduced autocorrelation to less than 0.1 at a time lag of 1. We can see this result in figure <a href="#5">5</a>. We used the Gelman-Rubin potential scale reduction factor to determine if we were observing favorable mixing. Generally, a value less than 1.1 indicates good mixing. In both of our dimensions, the Gelman-Rubin statistic was less than this threshold. We also calculated the Geweke statistic, which is used to indicate convergence. A value less than 2 indicates convergence and for our draws, this statistic was $\ll$ 1. We can also examine convergence by looking at the trace plots for the samples. As we see in figure <a href="#6">6</a> these appear stationary.

              <p></p>
              <div class="panel panel-info">
                <a name="5"></a>
                <div class="panel-heading">Figure 5</div>
                <div class="panel-body">
                  <img src="img/autocorr-latlong.png" width="100%" max-width="600" />
                </div>
                <div class="panel-footer">Autocorrelation for the samples from latitude and longitude</div>
              </div>

              <p></p>
              <div class="panel panel-info">
                <a name="6"></a>
                <div class="panel-heading">Figure 6</div>
                <div class="panel-body">
                  <img src="img/trace-plots.png" width="100%" max-width="600" />
                </div>
                <div class="panel-footer">Trace plots for draws from the latitude and longitude</div>
              </div>

              <h4>Events in time</h4>

              As a modeling assumption, we separate the dimensions of space and time as being independent. To model events in time across the country, we use an autoregressive Poisson GLM. While standard autoregressive models create a linear relation between a future value and a previous value,
              the Poisson GLM permits a linear relation between previous data and the mean of a Poisson distribution. This will allow us to retain the probabilistic interpretation of the events in time.

              In order to model events using a Poisson distribution, we must discretize our time dimension. We opted for month-long increments. The thinking behind this decision is that we want to use sample draws to run our aid optimizations. If a model like this were to be used in planning for future conflicts, having a month-long window for a plan seems like a good balance between precision and logistic concerns.

              The Autoregressive Poisson GLM model can be described as a log-linear relationship between the number of events of political violence and the mean of a Poisson distribution. We start with a timeseries,
              $\mathbf{X} = \{x_0, x_1,\ldots, x_N\}$, of $N$ counts of events at each discretized point in time. We also start with a lag $\Lambda$ that is the number of previous time steps to include in the model. We can now describe our features at time $t$ as the $\Lambda$ previous time steps:
              $\mathbf{X_{t, \Lambda}} = \{x_{t-\Lambda}, x_{t-\Lambda-1},\ldots, x_t\}$.

              The linear predictor in the autoregressive model at a time step is $\eta_t$, and it is related to the mean of the Poisson distribution,
              $\mu_t$, by its canonical link function, $\log$.

              $$\begin{aligned}
              \eta_t &= \mathbf{X}_{t, \Lambda}'\beta. \\
              \mu_t &= \log(\eta_t) = \log(\mathbf{X}_{t, \Lambda}'\beta) \\\end{aligned}$$

              Finally, the only parameter to the Poisson distribution is this mean, so the distribution of counts, $k$, at some time t+1 can be given by:

              $$\begin{aligned}
              p(k | \mu_t) &= \frac{\mu_t^k}{k!}e^{-\mu_t} \\
              p(k | \mathbf{X}_{t, \Lambda}, \beta) &= \frac{\log(\mathbf{X}_{t, \Lambda}'\beta)^k}{k!}e^{-\log(\mathbf{X}_{t, \Lambda}'\beta)} \end{aligned}$$

              We can fit this model by using Fisher scoring to calculate $\beta_\mathrm{MLE}$, the coefficients of the model. Figure <a href="#7">7</a> shows this model as compared to actual rates of conflict incidence.

              <p></p>
              <div class="panel panel-info">
                <a name="7"></a>
                <div class="panel-heading">Figure 7</div>
                <div class="panel-body">
                  <img src="img/poisson-regression.png" width="100%" max-width="600" />
                </div>
                <div class="panel-footer">The Poisson regression model.</div>
              </div>

              <h4>Putting together the spatial and temporal</h4>

              We can now use our draws over space and time to generate a simulation of future conflicts in Uganda. These simulated scenarios will serve as the basis of our aid delivery optimization. The combination of modeling conflict events in space and time along with optimizing aid delivery could prove helpful to organizations such as the Red Cross in ordering supplies, allocating staff and volunteers, and developing infrastructure.

              <h3>Optimizing humanitarian aid delivery</h3>

              In the second part of this project, we use the temporal/geospatial conflict occurence model in the first section as both inspiration for the aid delivery analogy and also as a source of randomly sampled data points representing geospatially distributed conflicts.

              <h4>The traveling salesman problem</h4>

              One question of particular interest is how to route emergency aid to locations where it is needed. For concreteness, let’s postulate a Red Cross medical or food supply caravan that originates from the organization’s in-country headquarters. This caravan wishes to visit all $n$ emergent locations in order to deliver needed supplies. They wish to do so in the most efficient manner possible.

              This is the traveling salesman problem (TSP), an optimization problem that is quite well known. It was first described in 1932 by Karl Menger and has been studied extensively ever since. Here is the traditional convex optimization specification of the problem:

              $$\begin{aligned}
              \min &\sum_{i=0}^n \sum_{j\ne i,j=0}^nc_{ij}x_{ij} &&  \\
              \mathrm{s.t.} & \\
              & x_{ij} \in \{0, 1\} && i,j=0, \cdots, n \\
              & \sum_{i=0,i\ne j}^n x_{ij} = 1 && j=0, \cdots, n \\
              & \sum_{j=0,j\ne i}^n x_{ij} = 1 && i=0, \cdots, n \\
              &u_i-u_j +nx_{ij} \le n-1 && 1 \le i \ne j \le n\end{aligned}$$

              As is clear from the constraints, this is an integer linear program (ILP) where:

              <ul>
                <li>$x_{ij}$ is a binary decision variable indicating whether we go from location $i$ to location $j$.</li>
                <li>$c_{ij}$ is the distance between location $i$ and location $j$. (Note: in our application, we deal with geospatial data on a large enough scale that the Euclidean distance is actually very imprecise. In order to model distances over the planet’s surface, we use the Haversine formula.)</li>
                <li>The objective function is the sum of the distances for routes that we decide to take.</li>
                <li>The final constraint ensures that all locations are visited once and only once.</li>
              </ul>

              The problem, of course, is that brute force solution of the TSP is $\mathcal{O}$$(n!)$. Traditional, deterministic algorithm approaches such as branch-and-bound or branch-and-cut are still impractical for larger numbers of nodes. In many cases, exhaustive search for global optimality is not even particularly helpful as long as the solution found is good enough. We will use simulated annealing (SA) to get acceptable solutions to the TSP.

              Figure <a href="#8">8</a> shows a sample draw of conflict data (the blue points), and a near-optimal TSP route found through 50,000 iterations of simulated annealing.

              <p></p>
              <div class="panel panel-info">
                <a name="8"></a>
                <div class="panel-heading">Figure 8</div>
                <div class="panel-body">
                  <img src="img/routing-vanilla-tsp.png" width="100%" max-width="600" />
                </div>
                <div class="panel-footer">Visiting all conflicts without reloading.</div>
              </div>

              <h4>Packing the aid truck &mdash; the Knapsack Problem</h4>

              We extend the TSP into a multi-objective optimization problem where the contents of the aid trucks also have an optimization component. Therein lies the knapsack problem: subject to a volume or weight constraint, and given that different locations might have very different needs such as food, vaccinations, or emergent medical supplies, which supplies do we pack on the trucks?

              Often, this problem is formulated such that you can only bring one of each item, but that does not make sense in our application. Rather, we want to be able to bring as many types of each type of aid as we think necessary, and we’ll assume that as many as desired are available to load on the trucks before starting out from HQ.  Here’s the unbounded version of the knapsack problem:

              $$\begin{aligned}
              \max &\sum_{i=1}^n v_i x_i &&  \\
              \mathrm{s.t.} & \\
              & x_i \in \mathbb{Z} \\
              & x_i \geq 0 \\
              & \sum_{i=1}^n w_ix_i \leq W\end{aligned}$$

              In this formulation:

              <ul>
                <li>$x_{i}$ is a zero or positive integer decision variable indicating how many units of item $i$ we load on the truck.</li>
                <li>$v_i$ is the utility we get from bringing along item $i$.</li>
                <li>$w_i$ is the weight of item $i$.</li>
                <li>$W$ is the maximum weight the truck can carry.</li>
              </ul>

              <h4>A brief detour for modeling assumptions</h4>

              Before we can optimize this aid delivery mechanism, we will need to decide a way to model humanitarian aid needs at a given conflict.

              Let us assume that there are $K$ distinct types of humanitarian aid to be delivered. (Without loss of generality, we will use three categories for all of our examples &mdash; perhaps we can think of them food aid, first aid supplies, and medicines for concreteness.) We can model each conflict’s aid needs as

              $$\boldsymbol x \sim \mathrm{Dir}(\boldsymbol \alpha)$$

              where $\boldsymbol \alpha$ parameterizes the distribution to generate vectors of length $K$ representing the relative proportions of needs. For example, in our three category example we might draw the vector $(0.11,\,0.66,\,0.23)$ for a certain conflict, meaning that 11% of the aid needed at this conflict is food aid, 66% is first aid supplies, and 23% is medicines. Now that we know the proportions for the given conflict, how might we turn this unitless vector into absolute amounts?

              For that reason, let’s assign each conflict a scaled size $s \in [1, 10]$ based on the number of casualties (a proxy for the severity of the conflict). We can use this size scalar to turn our proportion vector into a vector of absolute needs.

              It should be noted that both of these modeling methods for proportions and size are “plug-and-play” &mdash; because of purposely designed loose coupling in our model, these methods could trivially be replaced by a different method of calculating or predicting the needs of each conflict. For example, if an independent model was used to calculate each of $K$ needs based on the features of each conflict, those quantities could easily be plugged in to this model. Ultimately, the only quantities that our TSP/Knapsack model needs is an $n \times K$
              matrix of aid needs for $n$ cities and $K$ categories of aid.

              <h4>A new objective function to integrate TSP and Knapsack</h4>

              For the vanilla TSP, we simply try to minimize the total distance. Now that we are adding a new objective, we will need to integrate the two into a coherent loss function. Here is the function we will actually try to minimize in the combined TSP/Knapsack:

              $$L(\boldsymbol x) = \text{total distance} + \text{sum of squared aid shortfalls}$$

              The effect of squaring aid shortfalls acts as a weight, causing greater importance to be placed on minimizing this aspect of the problem first.
              Proposals wherein aid shortfalls occur are heavily penalized. As we will see in later graphs, once the SA algorithm is able to avoid all shortfalls and the concurrent massive loss function penalties, a much slower descent begins to take place wherein the distance is slowly optimized. See figure <a href="#10">10</a> for a depiction of this phenomenon.

              <h4>Implementing the Knapsack aspect</h4>

              Figure <a href="#9">9</a> shows the same draw of cities as in figure <a href="#8">8</a>, this time factoring in limited carrying capacity for aid supplies on the aid delivery mechanism and using our new loss function. As we can see, the huge penalty incurred when supplies run out quickly induces the simulated annealing algorithm to converge on a solution with multiple stops at HQ to reload.

              <p></p>
              <div class="panel panel-info">
                <a name="9"></a>
                <div class="panel-heading">Figure 9</div>
                <div class="panel-body">
                  <img src="img/routing-reloading.png" width="100%" max-width="600" />
                </div>
                <div class="panel-footer">Routing with reloading from capital city Kampala.</div>
              </div>

              <p></p>
              <div class="panel panel-info">
                <a name="10"></a>
                <div class="panel-heading">Figure 10</div>
                <div class="panel-body">
                  <img src="img/sorties-loss.png" width="100%" max-width="600" />
                </div>
                <div class="panel-footer">Loss function acceptances over 100,000 iterations.</div>
              </div>

              Figures <a href="#11">11</a>, <a href="#12">12</a>, and <a href="#13">13</a> use some uniformly distributed points on the $[0,50]$ plane to demonstrate how the proposed TSP/Knapsack routes converge as the number of iterations increases.

              <p></p>
              <div class="panel panel-info">
                <a name="11"></a>
                <div class="panel-heading">Figure 11</div>
                <div class="panel-body">
                  <img src="img/sorties-5000.png" width="100%" max-width="600" />
                </div>
                <div class="panel-footer">After 5,000 iterations.</div>
              </div>

              <p></p>
              <div class="panel panel-info">
                <a name="12"></a>
                <div class="panel-heading">Figure 12</div>
                <div class="panel-body">
                  <img src="img/sorties-20000.png" width="100%" max-width="600" />
                </div>
                <div class="panel-footer">After 20,000 iterations.</div>
              </div>

              <p></p>
              <div class="panel panel-info">
                <a name="13"></a>
                <div class="panel-heading">Figure 13</div>
                <div class="panel-body">
                  <img src="img/sorties-100000.png" width="100%" max-width="600" />
                </div>
                <div class="panel-footer">After 100,000 iterations.</div>
              </div>

              <h4>Finding the optimal site for the resupply location</h4>

              Our initial assumption was that the HQ was located in the capital city of Kampala. However, we should ask whether our HQ could be more conveniently located. We can answer this question by treating the reload location as another parameter and continuing to sample HQ locations using SA. Figure <a href="#14">14</a> shows the TSP/Knapsack optimized once again, this time using a the optimal HQ location, while figure <a href="#15">15</a> compares the loss function as each method converges to its best possible configuration.

              <p></p>
              <div class="panel panel-info">
                <a name="14"></a>
                <div class="panel-heading">Figure 14</div>
                <div class="panel-body">
                  <img src="img/routing-reloading-hq-optimal.png" width="100%" max-width="600" />
                </div>
                <div class="panel-footer">Routing with reloading from optimal HQ.</div>
              </div>

              <p></p>
              <div class="panel panel-info">
                <a name="15"></a>
                <div class="panel-heading">Figure 15</div>
                <div class="panel-body">
                  <img src="img/comparative-loss-plot.png" width="100%" max-width="600" />
                </div>
                <div class="panel-footer">Example loss function optimization convergence for $n=50$.</div>
              </div>

              <h3>Conclusions</h3>

              In each part of this problem, analytical solutions either do not exist (e.g. in the distribution of events) or are computationally infeasible (e.g. in the TSP/Knapsack optimizations). We found that using a metaheuristic such as SA converged on robust solutions in relatively short order. In the future, we would like to formulate our loss function based on real world data based on refugee locations and aid distribution requirements; our methodology would not change, but the solutions would be more useful for predictive tasks. Additionally, the separate models could be fit and incorporated which realistically model how much of each type of aid is needed at each conflict location. Future research might also include adding many more constraints or twists to the problem, and trying different stochastic optimization techniques such as genetic optimization, Tabu search, or ant colony optimization.

            </div>
          </div>

          <div class="footer">
            <p>P. Bull, I. Slavitt, 2014</p>
          </div>

        </div> <!-- /container -->


        <!-- Bootstrap core JavaScript ================================================== -->
        <!-- Placed at the end of the document so the pages load faster -->
        <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
        <script src="https://code.jquery.com/jquery.js"></script>
        <!-- Include all compiled plugins (below), or include individual files as needed -->
        <script src="//netdna.bootstrapcdn.com/bootstrap/3.0.2/js/bootstrap.min.js"></script>
      </body>
      </html>
